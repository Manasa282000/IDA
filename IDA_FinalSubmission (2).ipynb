{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "160cd084-5d6c-45c3-af3e-a8dd9b79c3ae",
   "metadata": {},
   "source": [
    "## Machine Learning Approach to Identify risk factors for Antepartum Hemorrhage (APH) \n",
    "\n",
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "da273bdc-b8a6-40bf-892a-b29a2ef52749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "940cb013-9ac6-4661-9d9f-23ffca8fda7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0992821-7c3d-47d4-9d58-a8125e29cdbe",
   "metadata": {},
   "source": [
    "### Reading the Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0b620925-0099-4575-88db-9e4bbbec4094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WHOWID</th>\n",
       "      <th>ORIG_ID</th>\n",
       "      <th>PARTICIPANT_ID</th>\n",
       "      <th>PW_AGE</th>\n",
       "      <th>PW_EDUCATION</th>\n",
       "      <th>PREV_SB</th>\n",
       "      <th>PREV_MIS</th>\n",
       "      <th>PREV_PTB</th>\n",
       "      <th>PREV_MULTIP</th>\n",
       "      <th>PREV_CS</th>\n",
       "      <th>...</th>\n",
       "      <th>DBP4</th>\n",
       "      <th>UDIP_PROT4</th>\n",
       "      <th>DEL_DATE</th>\n",
       "      <th>GAGEBRTH</th>\n",
       "      <th>TYPEDELIV</th>\n",
       "      <th>age_death_b1</th>\n",
       "      <th>age_death_b2</th>\n",
       "      <th>age_death_b3</th>\n",
       "      <th>APH</th>\n",
       "      <th>MAT_WEIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20-016580</td>\n",
       "      <td>16580</td>\n",
       "      <td>AMANHIT-20916</td>\n",
       "      <td>36</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-10-31</td>\n",
       "      <td>271.0</td>\n",
       "      <td>Normally through the vagina</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20-016683</td>\n",
       "      <td>16683</td>\n",
       "      <td>AMANHIT-22194</td>\n",
       "      <td>32</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-01-06</td>\n",
       "      <td>274.0</td>\n",
       "      <td>Normally through the vagina</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20-016685</td>\n",
       "      <td>16685</td>\n",
       "      <td>AMANHIT-22712</td>\n",
       "      <td>18</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>290.0</td>\n",
       "      <td>Normally through the vagina</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      WHOWID  ORIG_ID PARTICIPANT_ID  PW_AGE  PW_EDUCATION  PREV_SB  PREV_MIS  \\\n",
       "0  20-016580    16580  AMANHIT-20916      36          10.0        1         2   \n",
       "1  20-016683    16683  AMANHIT-22194      32          10.0        0         0   \n",
       "2  20-016685    16685  AMANHIT-22712      18           6.0        0         1   \n",
       "\n",
       "   PREV_PTB  PREV_MULTIP  PREV_CS  ...  DBP4  UDIP_PROT4   DEL_DATE  GAGEBRTH  \\\n",
       "0         0            1        0  ...  69.0         0.0 2014-10-31     271.0   \n",
       "1         0            0        0  ...  73.0         0.0 2015-01-06     274.0   \n",
       "2         0            0        0  ...  70.0         0.0 2015-01-31     290.0   \n",
       "\n",
       "                     TYPEDELIV  age_death_b1  age_death_b2  age_death_b3  APH  \\\n",
       "0  Normally through the vagina           NaN           NaN           NaN  0.0   \n",
       "1  Normally through the vagina           NaN           NaN           NaN  0.0   \n",
       "2  Normally through the vagina           NaN           NaN           NaN  0.0   \n",
       "\n",
       "  MAT_WEIGHT  \n",
       "0       45.8  \n",
       "1        NaN  \n",
       "2       68.0  \n",
       "\n",
       "[3 rows x 51 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the file path\n",
    "df = pd.read_excel(r\"C:\\Users\\Manasa Madabhushi\\Python_Projects\\4501_AMANHI_With_USG.xlsx\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "996ce4f6-675a-44b7-87bd-b2a14a5503b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4501 entries, 0 to 4500\n",
      "Data columns (total 51 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   WHOWID          4501 non-null   object        \n",
      " 1   ORIG_ID         4501 non-null   int64         \n",
      " 2   PARTICIPANT_ID  4501 non-null   object        \n",
      " 3   PW_AGE          4501 non-null   int64         \n",
      " 4   PW_EDUCATION    4485 non-null   float64       \n",
      " 5   PREV_SB         4501 non-null   int64         \n",
      " 6   PREV_MIS        4501 non-null   int64         \n",
      " 7   PREV_PTB        4501 non-null   int64         \n",
      " 8   PREV_MULTIP     4501 non-null   int64         \n",
      " 9   PREV_CS         4501 non-null   int64         \n",
      " 10  WEALTH_INDEX    4501 non-null   object        \n",
      " 11  SINGLE_TWIN     4501 non-null   int64         \n",
      " 12  GRAVIDITY       4501 non-null   int64         \n",
      " 13  PARITY          4501 non-null   int64         \n",
      " 14  LABOUR_HTN      4501 non-null   int64         \n",
      " 15  LABOUR_24       4501 non-null   int64         \n",
      " 16  BIRTH_OUTCOME   4501 non-null   int64         \n",
      " 17  BABY_SEX        4501 non-null   int64         \n",
      " 18  BIRTH_WEIGHT    3943 non-null   float64       \n",
      " 19  BABY_ID1        4501 non-null   object        \n",
      " 20  BIRTH_OUTCOME1  4501 non-null   int64         \n",
      " 21  BABY_SEX1       4501 non-null   int64         \n",
      " 22  BIRTH_WEIGHT1   3943 non-null   float64       \n",
      " 23  BABY_ID2        90 non-null     object        \n",
      " 24  BIRTH_OUTCOME2  90 non-null     float64       \n",
      " 25  BABY_SEX2       90 non-null     float64       \n",
      " 26  BIRTH_WEIGHT2   78 non-null     float64       \n",
      " 27  BABY_ID3        2 non-null      object        \n",
      " 28  BIRTH_OUTCOME3  2 non-null      float64       \n",
      " 29  BABY_SEX3       2 non-null      float64       \n",
      " 30  BIRTH_WEIGHT3   2 non-null      float64       \n",
      " 31  SBP1            4302 non-null   float64       \n",
      " 32  DBP1            4302 non-null   float64       \n",
      " 33  UDIP_PROT1      4302 non-null   float64       \n",
      " 34  SBP2            4209 non-null   float64       \n",
      " 35  DBP2            4209 non-null   float64       \n",
      " 36  UDIP_PROT2      4209 non-null   float64       \n",
      " 37  SBP3            4107 non-null   float64       \n",
      " 38  DBP3            4107 non-null   float64       \n",
      " 39  UDIP_PROT3      4107 non-null   float64       \n",
      " 40  SBP4            3843 non-null   float64       \n",
      " 41  DBP4            3843 non-null   float64       \n",
      " 42  UDIP_PROT4      3843 non-null   float64       \n",
      " 43  DEL_DATE        4366 non-null   datetime64[ns]\n",
      " 44  GAGEBRTH        4366 non-null   float64       \n",
      " 45  TYPEDELIV       4277 non-null   object        \n",
      " 46  age_death_b1    87 non-null     float64       \n",
      " 47  age_death_b2    12 non-null     float64       \n",
      " 48  age_death_b3    0 non-null      float64       \n",
      " 49  APH             4414 non-null   float64       \n",
      " 50  MAT_WEIGHT      4170 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(27), int64(16), object(7)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef7d44-af55-4c05-8652-224b3850793f",
   "metadata": {},
   "source": [
    "### Drop the APH rows which are Empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "505fc2c8-64a8-40b8-b1a8-9aec4b3470c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOWID               0\n",
      "ORIG_ID              0\n",
      "PARTICIPANT_ID       0\n",
      "PW_AGE               0\n",
      "PW_EDUCATION        16\n",
      "PREV_SB              0\n",
      "PREV_MIS             0\n",
      "PREV_PTB             0\n",
      "PREV_MULTIP          0\n",
      "PREV_CS              0\n",
      "WEALTH_INDEX         0\n",
      "SINGLE_TWIN          0\n",
      "GRAVIDITY            0\n",
      "PARITY               0\n",
      "LABOUR_HTN           0\n",
      "LABOUR_24            0\n",
      "BIRTH_OUTCOME        0\n",
      "BABY_SEX             0\n",
      "BIRTH_WEIGHT       558\n",
      "BABY_ID1             0\n",
      "BIRTH_OUTCOME1       0\n",
      "BABY_SEX1            0\n",
      "BIRTH_WEIGHT1      558\n",
      "BABY_ID2          4411\n",
      "BIRTH_OUTCOME2    4411\n",
      "BABY_SEX2         4411\n",
      "BIRTH_WEIGHT2     4423\n",
      "BABY_ID3          4499\n",
      "BIRTH_OUTCOME3    4499\n",
      "BABY_SEX3         4499\n",
      "BIRTH_WEIGHT3     4499\n",
      "SBP1               199\n",
      "DBP1               199\n",
      "UDIP_PROT1         199\n",
      "SBP2               292\n",
      "DBP2               292\n",
      "UDIP_PROT2         292\n",
      "SBP3               394\n",
      "DBP3               394\n",
      "UDIP_PROT3         394\n",
      "SBP4               658\n",
      "DBP4               658\n",
      "UDIP_PROT4         658\n",
      "DEL_DATE           135\n",
      "GAGEBRTH           135\n",
      "TYPEDELIV          224\n",
      "age_death_b1      4414\n",
      "age_death_b2      4489\n",
      "age_death_b3      4501\n",
      "APH                 87\n",
      "MAT_WEIGHT         331\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "print(df.isnull().sum())  # This will show the count of missing values in each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1c89bd85-bb98-4a8c-980d-faca65d21c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['APH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370f95f-7413-4a6b-ad1d-14711cf5b78a",
   "metadata": {},
   "source": [
    "### Replacing the -88 and -77 as NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "85cd4182-645a-4dc3-8dcd-cea2720827d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace([-88, -77], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ff4060bc-92d1-4ec4-aab3-3bf4296d9bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['WHOWID', 'ORIG_ID', 'PARTICIPANT_ID', 'PW_AGE', 'PW_EDUCATION',\n",
       "       'PREV_SB', 'PREV_MIS', 'PREV_PTB', 'PREV_MULTIP', 'PREV_CS',\n",
       "       'WEALTH_INDEX', 'SINGLE_TWIN', 'GRAVIDITY', 'PARITY', 'LABOUR_HTN',\n",
       "       'LABOUR_24', 'BIRTH_OUTCOME', 'BABY_SEX', 'BIRTH_WEIGHT', 'BABY_ID1',\n",
       "       'BIRTH_OUTCOME1', 'BABY_SEX1', 'BIRTH_WEIGHT1', 'BABY_ID2',\n",
       "       'BIRTH_OUTCOME2', 'BABY_SEX2', 'BIRTH_WEIGHT2', 'BABY_ID3',\n",
       "       'BIRTH_OUTCOME3', 'BABY_SEX3', 'BIRTH_WEIGHT3', 'SBP1', 'DBP1',\n",
       "       'UDIP_PROT1', 'SBP2', 'DBP2', 'UDIP_PROT2', 'SBP3', 'DBP3',\n",
       "       'UDIP_PROT3', 'SBP4', 'DBP4', 'UDIP_PROT4', 'DEL_DATE', 'GAGEBRTH',\n",
       "       'TYPEDELIV', 'age_death_b1', 'age_death_b2', 'age_death_b3', 'APH',\n",
       "       'MAT_WEIGHT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471dc29b-c4fa-4768-a45d-224d979470ff",
   "metadata": {},
   "source": [
    "### Drop the columns which are related to the Post Pregancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2dfe111a-9610-492f-839c-26196aa4ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['ORIG_ID','PARTICIPANT_ID','BIRTH_OUTCOME','BABY_SEX','BIRTH_WEIGHT','LABOUR_HTN','GAGEBRTH','BABY_ID1','BABY_ID2','BABY_ID3','WHOWID','BIRTH_OUTCOME2','BIRTH_WEIGHT2','BIRTH_OUTCOME3','BABY_SEX3','BIRTH_WEIGHT3','WEALTH_INDEX','DEL_DATE','age_death_b1','age_death_b2','age_death_b3','BABY_SEX2','TYPEDELIV','BIRTH_OUTCOME1','BABY_SEX1','BIRTH_WEIGHT1'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721db4a7-c7b1-498f-a9fb-929c591bd1b2",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Feature Engineering separately for the APH and Non-APH \n",
    "#### *Splitting the APH and Non-APH Cases to perform the Data Preprocessing \n",
    "#### *Filling the missing values\n",
    "#### *Removal of Outliers Using the HDBSCAN and IQR \n",
    "#### *Feature Selection by the Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7d483732-8322-433e-8081-253a23a77a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonAPH = df[df[\"APH\"] == 0]\n",
    "df_APH = df[df[\"APH\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99bcae5-a392-4e1a-8c06-5262e1a0e391",
   "metadata": {},
   "source": [
    "### Imputing the missing values Numerical & Categorical columns for APH and Non-APH case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bde1ef37-343c-4497-8abf-c9b47eecf42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in df_nonAPH after imputation:\n",
      "SBP1            0\n",
      "DBP1            0\n",
      "UDIP_PROT1      0\n",
      "SBP2            0\n",
      "DBP2            0\n",
      "UDIP_PROT2      0\n",
      "SBP3            0\n",
      "DBP3            0\n",
      "UDIP_PROT3      0\n",
      "SBP4            0\n",
      "DBP4            0\n",
      "UDIP_PROT4      0\n",
      "MAT_WEIGHT      0\n",
      "GRAVIDITY       0\n",
      "PARITY          0\n",
      "PW_EDUCATION    0\n",
      "LABOUR_24       0\n",
      "SINGLE_TWIN     0\n",
      "PREV_SB         0\n",
      "PREV_MIS        0\n",
      "PREV_PTB        0\n",
      "PREV_MULTIP     0\n",
      "PREV_CS         0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in df_APH after imputation:\n",
      "SBP1            0\n",
      "DBP1            0\n",
      "UDIP_PROT1      0\n",
      "SBP2            0\n",
      "DBP2            0\n",
      "UDIP_PROT2      0\n",
      "SBP3            0\n",
      "DBP3            0\n",
      "UDIP_PROT3      0\n",
      "SBP4            0\n",
      "DBP4            0\n",
      "UDIP_PROT4      0\n",
      "MAT_WEIGHT      0\n",
      "GRAVIDITY       0\n",
      "PARITY          0\n",
      "PW_EDUCATION    0\n",
      "LABOUR_24       0\n",
      "SINGLE_TWIN     0\n",
      "PREV_SB         0\n",
      "PREV_MIS        0\n",
      "PREV_PTB        0\n",
      "PREV_MULTIP     0\n",
      "PREV_CS         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_nonAPH[col].fillna(median_value_nonAPH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_APH[col].fillna(median_value_APH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_nonAPH[col].fillna(median_value_nonAPH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_APH[col].fillna(median_value_APH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_nonAPH[col].fillna(median_value_nonAPH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_APH[col].fillna(median_value_APH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_nonAPH[col].fillna(median_value_nonAPH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_APH[col].fillna(median_value_APH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_nonAPH[col].fillna(median_value_nonAPH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_APH[col].fillna(median_value_APH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_nonAPH[col].fillna(median_value_nonAPH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_APH[col].fillna(median_value_APH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_nonAPH[col].fillna(median_value_nonAPH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_APH[col].fillna(median_value_APH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_nonAPH[col].fillna(median_value_nonAPH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_APH[col].fillna(median_value_APH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_nonAPH[col].fillna(median_value_nonAPH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_APH[col].fillna(median_value_APH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_nonAPH[col].fillna(median_value_nonAPH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_APH[col].fillna(median_value_APH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_nonAPH[col].fillna(median_value_nonAPH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_APH[col].fillna(median_value_APH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_nonAPH[col].fillna(median_value_nonAPH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_APH[col].fillna(median_value_APH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_nonAPH[col].fillna(median_value_nonAPH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_APH[col].fillna(median_value_APH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_nonAPH[col].fillna(median_value_nonAPH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_APH[col].fillna(median_value_APH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_nonAPH[col].fillna(median_value_nonAPH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_APH[col].fillna(median_value_APH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_nonAPH[col].fillna(mode_value_nonAPH, inplace=True)\n",
      "C:\\Users\\Manasa Madabhushi\\AppData\\Local\\Temp\\ipykernel_25212\\3854082890.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_APH[col].fillna(mode_value_APH, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Columns to be imputed\n",
    "numerical_columns = ['SBP1', 'DBP1', 'UDIP_PROT1', 'SBP2', 'DBP2', 'UDIP_PROT2', \n",
    "                     'SBP3', 'DBP3', 'UDIP_PROT3', 'SBP4', 'DBP4', 'UDIP_PROT4', \n",
    "                     'MAT_WEIGHT', 'GRAVIDITY', 'PARITY']\n",
    "\n",
    "categorical_columns = ['PW_EDUCATION', 'LABOUR_24', 'SINGLE_TWIN', 'PREV_SB', \n",
    "                       'PREV_MIS', 'PREV_PTB', 'PREV_MULTIP', 'PREV_CS']\n",
    "\n",
    "# Fill missing values in numerical columns using median\n",
    "for col in numerical_columns:\n",
    "    median_value_nonAPH = df_nonAPH[col].median()\n",
    "    df_nonAPH[col].fillna(median_value_nonAPH, inplace=True)\n",
    "    \n",
    "    median_value_APH = df_APH[col].median()\n",
    "    df_APH[col].fillna(median_value_APH, inplace=True)\n",
    "\n",
    "# Fill missing values in categorical columns using mode\n",
    "for col in categorical_columns:\n",
    "    mode_value_nonAPH = df_nonAPH[col].mode()[0]  # Get the most frequent value\n",
    "    df_nonAPH[col].fillna(mode_value_nonAPH, inplace=True)\n",
    "    \n",
    "    mode_value_APH = df_APH[col].mode()[0]  # Get the most frequent value\n",
    "    df_APH[col].fillna(mode_value_APH, inplace=True)\n",
    "\n",
    "# Check for remaining missing values\n",
    "print(\"Missing values in df_nonAPH after imputation:\")\n",
    "print(df_nonAPH[numerical_columns + categorical_columns].isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in df_APH after imputation:\")\n",
    "print(df_APH[numerical_columns + categorical_columns].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d242e0d2-4526-48d3-aba8-d59a13c9ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_set_nonAPH, train_set_APH, test_set_nonAPH, test_set_APH are pandas DataFrames\n",
    "\n",
    "# Merge the train datasets\n",
    "df = pd.concat([df_nonAPH, df_APH], axis=0)\n",
    "\n",
    "# Reset the index (optional, to clean up any duplicate indices after concatenation)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9cd3b74b-ccdc-405c-95a8-eea7dab88e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "APH\n",
       "0.0    4243\n",
       "1.0     171\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['APH'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e4171-c428-4ac9-adc0-711320001ae4",
   "metadata": {},
   "source": [
    "## One hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8b65f17d-ffd5-4b97-b44f-2f2feec9d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding with pd.get_dummies\n",
    "df = pd.get_dummies(df, columns=['LABOUR_24'], drop_first=True)\n",
    "\n",
    "\n",
    "# Verify and explicitly convert the encoded column to integer\n",
    "df['LABOUR_24_1.0'] = df['LABOUR_24_1.0'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06380b99-9d7a-4b4c-b6af-af5f4f574c58",
   "metadata": {},
   "source": [
    "## Outliers removal by HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "33d1eb18-3189-4154-9dba-a2d0271eb20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution After Outlier Removal:\n",
      "APH\n",
      "0.0    3288\n",
      "1.0     136\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Step 1: Standardize the Data\n",
    "# -----------------------------------------------------------------\n",
    "# Assuming `df` is your dataframe and `APH` is the target column\n",
    "X = df.drop(columns=['APH'])  # Exclude the target column for scaling\n",
    "y = df['APH']  # Target column\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Step 2: Apply HDBSCAN for Outlier Detection\n",
    "# -----------------------------------------------------------------\n",
    "hdbscan = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_epsilon=0.5)\n",
    "cluster_labels = hdbscan.fit_predict(X_scaled)\n",
    "\n",
    "# Identify outliers (labeled as -1 by HDBSCAN) \n",
    "non_outliers_mask = cluster_labels != -1\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Step 3: Create Cleaned Dataset\n",
    "# -----------------------------------------------------------------\n",
    "# Combine back the non-outlier data with the target column\n",
    "X_cleaned = X[non_outliers_mask]\n",
    "y_cleaned = y[non_outliers_mask]\n",
    "\n",
    "\n",
    "# Create the cleaned DataFrame\n",
    "df_cleaned = pd.concat([X_cleaned, y_cleaned], axis=1)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Step 4: Check Class Distribution\n",
    "# -----------------------------------------------------------------\n",
    "class_distribution = df_cleaned['APH'].value_counts()\n",
    "print(\"Class Distribution After Outlier Removal:\")\n",
    "print(class_distribution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f10681-2a6b-460b-a1f4-f59399b34a02",
   "metadata": {},
   "source": [
    "## Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f3cdf892-063b-48d6-b079-755ecf3cb9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2739, 24)\n",
      "X_test shape: (685, 24)\n",
      "y_train shape: (2739,)\n",
      "y_test shape: (685,)\n",
      "\n",
      "Class distribution in y_train:\n",
      "APH\n",
      "0.0    0.960204\n",
      "1.0    0.039796\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution in y_test:\n",
      "APH\n",
      "0.0    0.960584\n",
      "1.0    0.039416\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = X_cleaned\n",
    "y = y_cleaned\n",
    "\n",
    "# Perform the stratified train-test split\n",
    "X_train_Clustering, X_test_Clustering, y_train_Clustering, y_test_Clustering = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(f\"X_train shape: {X_train_Clustering.shape}\")\n",
    "print(f\"X_test shape: {X_test_Clustering.shape}\")\n",
    "print(f\"y_train shape: {y_train_Clustering.shape}\")\n",
    "print(f\"y_test shape: {y_test_Clustering.shape}\")\n",
    "\n",
    "# Check class distribution in the train and test splits\n",
    "print(\"\\nClass distribution in y_train:\")\n",
    "print(y_train_Clustering.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nClass distribution in y_test:\")\n",
    "print(y_test_Clustering.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8dbba2-597e-4cfb-a815-64888ba64018",
   "metadata": {},
   "source": [
    "## Standarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9a889e97-f47b-4605-8c31-ade515f4a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming X_train_Clustering and X_test_Clustering are your training and test data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform both train and test data\n",
    "X_train_Clustering_scaled = scaler.fit_transform(X_train_Clustering)\n",
    "X_test_Clustering_scaled = scaler.transform(X_test_Clustering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9071654c-e716-49b1-9a4f-33f6a8a1bb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2739 entries, 0 to 2738\n",
      "Data columns (total 24 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   PW_AGE         2739 non-null   float64\n",
      " 1   PW_EDUCATION   2739 non-null   float64\n",
      " 2   PREV_SB        2739 non-null   float64\n",
      " 3   PREV_MIS       2739 non-null   float64\n",
      " 4   PREV_PTB       2739 non-null   float64\n",
      " 5   PREV_MULTIP    2739 non-null   float64\n",
      " 6   PREV_CS        2739 non-null   float64\n",
      " 7   SINGLE_TWIN    2739 non-null   float64\n",
      " 8   GRAVIDITY      2739 non-null   float64\n",
      " 9   PARITY         2739 non-null   float64\n",
      " 10  SBP1           2739 non-null   float64\n",
      " 11  DBP1           2739 non-null   float64\n",
      " 12  UDIP_PROT1     2739 non-null   float64\n",
      " 13  SBP2           2739 non-null   float64\n",
      " 14  DBP2           2739 non-null   float64\n",
      " 15  UDIP_PROT2     2739 non-null   float64\n",
      " 16  SBP3           2739 non-null   float64\n",
      " 17  DBP3           2739 non-null   float64\n",
      " 18  UDIP_PROT3     2739 non-null   float64\n",
      " 19  SBP4           2739 non-null   float64\n",
      " 20  DBP4           2739 non-null   float64\n",
      " 21  UDIP_PROT4     2739 non-null   float64\n",
      " 22  MAT_WEIGHT     2739 non-null   float64\n",
      " 23  LABOUR_24_1.0  2739 non-null   float64\n",
      "dtypes: float64(24)\n",
      "memory usage: 513.7 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 685 entries, 0 to 684\n",
      "Data columns (total 24 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   PW_AGE         685 non-null    float64\n",
      " 1   PW_EDUCATION   685 non-null    float64\n",
      " 2   PREV_SB        685 non-null    float64\n",
      " 3   PREV_MIS       685 non-null    float64\n",
      " 4   PREV_PTB       685 non-null    float64\n",
      " 5   PREV_MULTIP    685 non-null    float64\n",
      " 6   PREV_CS        685 non-null    float64\n",
      " 7   SINGLE_TWIN    685 non-null    float64\n",
      " 8   GRAVIDITY      685 non-null    float64\n",
      " 9   PARITY         685 non-null    float64\n",
      " 10  SBP1           685 non-null    float64\n",
      " 11  DBP1           685 non-null    float64\n",
      " 12  UDIP_PROT1     685 non-null    float64\n",
      " 13  SBP2           685 non-null    float64\n",
      " 14  DBP2           685 non-null    float64\n",
      " 15  UDIP_PROT2     685 non-null    float64\n",
      " 16  SBP3           685 non-null    float64\n",
      " 17  DBP3           685 non-null    float64\n",
      " 18  UDIP_PROT3     685 non-null    float64\n",
      " 19  SBP4           685 non-null    float64\n",
      " 20  DBP4           685 non-null    float64\n",
      " 21  UDIP_PROT4     685 non-null    float64\n",
      " 22  MAT_WEIGHT     685 non-null    float64\n",
      " 23  LABOUR_24_1.0  685 non-null    float64\n",
      "dtypes: float64(24)\n",
      "memory usage: 128.6 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the NumPy array to a DataFrame (assuming feature names are stored in a list)\n",
    "X_train_Clustering_scaled_df = pd.DataFrame(X_train_Clustering_scaled, columns=X_train_Clustering.columns)\n",
    "X_test_Clustering_scaled_df = pd.DataFrame(X_test_Clustering_scaled, columns=X_test_Clustering.columns)\n",
    "\n",
    "# Now you can use .info() with the DataFrame\n",
    "X_train_Clustering_scaled_df.info()\n",
    "X_test_Clustering_scaled_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b1890e-ab42-47eb-847a-6a0d91a8e9ae",
   "metadata": {},
   "source": [
    "## Feature Selection by Mutual Information HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9075fc6b-42ca-422d-b99e-aa94445228a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information Scores for Features:\n",
      "          Feature  Mutual Information\n",
      "16           SBP3            0.044910\n",
      "20           DBP4            0.030206\n",
      "22     MAT_WEIGHT            0.016817\n",
      "17           DBP3            0.009511\n",
      "14           DBP2            0.008991\n",
      "19           SBP4            0.005250\n",
      "3        PREV_MIS            0.005147\n",
      "23  LABOUR_24_1.0            0.003165\n",
      "4        PREV_PTB            0.002365\n",
      "7     SINGLE_TWIN            0.001967\n",
      "5     PREV_MULTIP            0.001290\n",
      "8       GRAVIDITY            0.000967\n",
      "11           DBP1            0.000816\n",
      "0          PW_AGE            0.000351\n",
      "13           SBP2            0.000192\n",
      "15     UDIP_PROT2            0.000000\n",
      "1    PW_EDUCATION            0.000000\n",
      "10           SBP1            0.000000\n",
      "18     UDIP_PROT3            0.000000\n",
      "9          PARITY            0.000000\n",
      "6         PREV_CS            0.000000\n",
      "21     UDIP_PROT4            0.000000\n",
      "2         PREV_SB            0.000000\n",
      "12     UDIP_PROT1            0.000000\n",
      "\n",
      "Top 5 Selected Features based on Mutual Information:\n",
      "['SBP3' 'DBP4' 'MAT_WEIGHT' 'DBP3' 'DBP2']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming X_train_Clustering_scaled and y_train_Clustering are already defined\n",
    "\n",
    "# Step 1: Calculate Mutual Information between features and the target\n",
    "mutual_info = mutual_info_classif(X_train_Clustering_scaled_df, y_train_Clustering)\n",
    "\n",
    "# Step 2: Create a DataFrame for better visualization of feature importance\n",
    "mutual_info_df = pd.DataFrame({\n",
    "    'Feature': X_train_Clustering_scaled_df.columns,\n",
    "    'Mutual Information': mutual_info\n",
    "})\n",
    "\n",
    "# Step 3: Sort the features by Mutual Information in descending order\n",
    "mutual_info_df = mutual_info_df.sort_values(by='Mutual Information', ascending=False)\n",
    "\n",
    "# Step 4: Display the results\n",
    "print(\"Mutual Information Scores for Features:\")\n",
    "print(mutual_info_df)\n",
    "\n",
    "# Optional: Select top N features based on Mutual Information\n",
    "top_n_features = mutual_info_df['Feature'].head(5).values  # Selecting top 5 features (adjust as needed)\n",
    "\n",
    "print(\"\\nTop 5 Selected Features based on Mutual Information:\")\n",
    "print(top_n_features)\n",
    "\n",
    "# You can now use these top features for further processing or training your model\n",
    "X_train_selected = X_train_Clustering_scaled[:, mutual_info_df['Feature'].isin(top_n_features)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09b099e-5fae-4b75-aa2f-2edee5b8d03f",
   "metadata": {},
   "source": [
    "### Ensembled Model With all the sampling for the Balanced bagging and all other --- HDBSACN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24de33b8-499c-4b37-a519-9eaf1d60f195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble Model Recall on Train Data (Class-wise): [0.75285171 0.90825688]\n",
      "Ensemble Model Recall on Test Data (Class-wise): [0.72036474 0.59259259]\n",
      "Ensemble Model F1-score on Train Data (Class-wise): [0.85714286 0.23076923]\n",
      "Ensemble Model F1-score on Test Data (Class-wise): [0.82939633 0.14096916]\n",
      "\n",
      "Ensemble Confusion Matrix (Test Data):\n",
      "[[474 184]\n",
      " [ 11  16]]\n",
      "\n",
      "Ensemble Classification Report (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.72      0.83       658\n",
      "         1.0       0.08      0.59      0.14        27\n",
      "\n",
      "    accuracy                           0.72       685\n",
      "   macro avg       0.53      0.66      0.49       685\n",
      "weighted avg       0.94      0.72      0.80       685\n",
      "\n",
      "ROC AUC Score: 0.7253743104806935\n",
      "\n",
      "Confusion Matrix plot saved at: ensemble_confusion_matrix.png\n",
      "ROC Curve plot saved at: ensemble_roc_curve.png\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, recall_score, f1_score\n",
    "from imblearn.ensemble import EasyEnsembleClassifier, RUSBoostClassifier, BalancedBaggingClassifier, BalancedRandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming X_train_Clustering_scaled_df, X_test_Clustering_scaled_df, y_train_Clustering, y_test_Clustering are already defined\n",
    "\n",
    "# Select the top features (assuming they are in the dataset)\n",
    "selected_features = ['SBP3', 'DBP4', 'MAT_WEIGHT', 'DBP2', 'DBP3']\n",
    "\n",
    "# Select only the relevant features from X_train and X_test\n",
    "X_train_selected = X_train_Clustering_scaled_df[selected_features]\n",
    "X_test_selected = X_test_Clustering_scaled_df[selected_features]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# Define individual models\n",
    "models = {\n",
    "    'EasyEnsembleClassifier': EasyEnsembleClassifier(random_state=42),\n",
    "    'RUSBoostClassifier': RUSBoostClassifier(random_state=42),\n",
    "    'BalancedBaggingClassifier': BalancedBaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=10), \n",
    "        n_estimators=100, \n",
    "        random_state=42\n",
    "    ),\n",
    "    'BalancedRandomForestClassifier': BalancedRandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Create an ensemble model using Voting Classifier (Majority Voting)\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('easy_ensemble', models['EasyEnsembleClassifier']),\n",
    "    ('rusboost', models['RUSBoostClassifier']),\n",
    "    ('balanced_bagging', models['BalancedBaggingClassifier']),\n",
    "    ('balanced_rf', models['BalancedRandomForestClassifier'])\n",
    "], voting='soft')  # Use 'soft' voting for probability averaging\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble_model.fit(X_train_scaled, y_train_Clustering)\n",
    "\n",
    "# Make predictions using the ensemble model\n",
    "y_pred_train_ensemble = ensemble_model.predict(X_train_scaled)\n",
    "y_pred_test_ensemble = ensemble_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate recall and F1-score for the ensemble (class-wise)\n",
    "recall_train_ensemble = recall_score(y_train_Clustering, y_pred_train_ensemble, average=None)  # No averaging, class-wise\n",
    "recall_test_ensemble = recall_score(y_test_Clustering, y_pred_test_ensemble, average=None)  # No averaging, class-wise\n",
    "f1_train_ensemble = f1_score(y_train_Clustering, y_pred_train_ensemble, average=None)  # No averaging, class-wise\n",
    "f1_test_ensemble = f1_score(y_test_Clustering, y_pred_test_ensemble, average=None)  # No averaging, class-wise\n",
    "\n",
    "print(f\"\\nEnsemble Model Recall on Train Data (Class-wise): {recall_train_ensemble}\")\n",
    "print(f\"Ensemble Model Recall on Test Data (Class-wise): {recall_test_ensemble}\")\n",
    "print(f\"Ensemble Model F1-score on Train Data (Class-wise): {f1_train_ensemble}\")\n",
    "print(f\"Ensemble Model F1-score on Test Data (Class-wise): {f1_test_ensemble}\")\n",
    "\n",
    "# Confusion Matrix for Ensemble Test Data\n",
    "cm = confusion_matrix(y_test_Clustering, y_pred_test_ensemble)\n",
    "\n",
    "print(\"\\nEnsemble Confusion Matrix (Test Data):\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nEnsemble Classification Report (Test Data):\")\n",
    "print(classification_report(y_test_Clustering, y_pred_test_ensemble))\n",
    "\n",
    "# ROC Curve and AUC for Ensemble\n",
    "fpr, tpr, thresholds = roc_curve(y_test_Clustering, ensemble_model.predict_proba(X_test_scaled)[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(f\"ROC AUC Score: {roc_auc}\")\n",
    "\n",
    "# Plot colored Confusion Matrix using Seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, \n",
    "            xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.title(f\"Ensemble Confusion Matrix\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save Confusion Matrix plot\n",
    "confusion_matrix_file = \"ensemble_confusion_matrix.png\"\n",
    "plt.savefig(confusion_matrix_file)\n",
    "plt.close()\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line (random classifier)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save ROC Curve plot\n",
    "roc_curve_file = \"ensemble_roc_curve.png\"\n",
    "plt.savefig(roc_curve_file)\n",
    "plt.close()\n",
    "\n",
    "# Print file paths for downloading\n",
    "print(f\"\\nConfusion Matrix plot saved at: {confusion_matrix_file}\")\n",
    "print(f\"ROC Curve plot saved at: {roc_curve_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4011e1e7-0d09-499f-ba92-e96aa2667ae7",
   "metadata": {},
   "source": [
    "### Ensembled Model With all the sampling for the Lightbgm and random forest --- HDBSACN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7ffe3e0c-b6c0-482e-9d64-ca7eb1d58141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RandomOverSampler...\n",
      "Fitting 2 folds for each of 64 candidates, totalling 128 fits\n",
      "[LightGBM] [Info] Number of positive: 2630, number of negative: 2630\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 436\n",
      "[LightGBM] [Info] Number of data points in the train set: 5260, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'ensemble__lightgbm__learning_rate': 0.01, 'ensemble__lightgbm__max_depth': -1, 'ensemble__lightgbm__n_estimators': 100, 'ensemble__lightgbm__num_leaves': 31, 'ensemble__random_forest__max_depth': 10, 'ensemble__random_forest__n_estimators': 250}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.98      0.98       658\n",
      "         1.0       0.36      0.30      0.33        27\n",
      "\n",
      "    accuracy                           0.95       685\n",
      "   macro avg       0.67      0.64      0.65       685\n",
      "weighted avg       0.95      0.95      0.95       685\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ROC Curve plot for RandomOverSampler saved as roc_curve_RandomOverSampler_HDBSCAN.png\n",
      "Using SMOTE...\n",
      "Fitting 2 folds for each of 64 candidates, totalling 128 fits\n",
      "[LightGBM] [Info] Number of positive: 1578, number of negative: 2630\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1270\n",
      "[LightGBM] [Info] Number of data points in the train set: 4208, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'ensemble__lightgbm__learning_rate': 0.01, 'ensemble__lightgbm__max_depth': 10, 'ensemble__lightgbm__n_estimators': 100, 'ensemble__lightgbm__num_leaves': 31, 'ensemble__random_forest__max_depth': 10, 'ensemble__random_forest__n_estimators': 250}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.98      0.98       658\n",
      "         1.0       0.41      0.33      0.37        27\n",
      "\n",
      "    accuracy                           0.95       685\n",
      "   macro avg       0.69      0.66      0.67       685\n",
      "weighted avg       0.95      0.95      0.95       685\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ROC Curve plot for SMOTE saved as roc_curve_SMOTE_HDBSCAN.png\n",
      "Using SMOTENC...\n",
      "Fitting 2 folds for each of 64 candidates, totalling 128 fits\n",
      "[LightGBM] [Info] Number of positive: 1578, number of negative: 2630\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000291 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1055\n",
      "[LightGBM] [Info] Number of data points in the train set: 4208, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'ensemble__lightgbm__learning_rate': 0.01, 'ensemble__lightgbm__max_depth': 10, 'ensemble__lightgbm__n_estimators': 100, 'ensemble__lightgbm__num_leaves': 31, 'ensemble__random_forest__max_depth': 10, 'ensemble__random_forest__n_estimators': 100}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.96      0.97       658\n",
      "         1.0       0.27      0.33      0.30        27\n",
      "\n",
      "    accuracy                           0.94       685\n",
      "   macro avg       0.62      0.65      0.63       685\n",
      "weighted avg       0.94      0.94      0.94       685\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ROC Curve plot for SMOTENC saved as roc_curve_SMOTENC_HDBSCAN.png\n",
      "Using SMOTEN...\n",
      "Fitting 2 folds for each of 64 candidates, totalling 128 fits\n",
      "[LightGBM] [Info] Number of positive: 1578, number of negative: 2630\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000291 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 434\n",
      "[LightGBM] [Info] Number of data points in the train set: 4208, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'ensemble__lightgbm__learning_rate': 0.1, 'ensemble__lightgbm__max_depth': -1, 'ensemble__lightgbm__n_estimators': 200, 'ensemble__lightgbm__num_leaves': 50, 'ensemble__random_forest__max_depth': 10, 'ensemble__random_forest__n_estimators': 250}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.98       658\n",
      "         1.0       0.70      0.26      0.38        27\n",
      "\n",
      "    accuracy                           0.97       685\n",
      "   macro avg       0.84      0.63      0.68       685\n",
      "weighted avg       0.96      0.97      0.96       685\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ROC Curve plot for SMOTEN saved as roc_curve_SMOTEN_HDBSCAN.png\n",
      "Using ADASYN...\n",
      "Fitting 2 folds for each of 64 candidates, totalling 128 fits\n",
      "[LightGBM] [Info] Number of positive: 1577, number of negative: 2630\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1272\n",
      "[LightGBM] [Info] Number of data points in the train set: 4207, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'ensemble__lightgbm__learning_rate': 0.01, 'ensemble__lightgbm__max_depth': -1, 'ensemble__lightgbm__n_estimators': 200, 'ensemble__lightgbm__num_leaves': 31, 'ensemble__random_forest__max_depth': 10, 'ensemble__random_forest__n_estimators': 100}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.97      0.97       658\n",
      "         1.0       0.35      0.33      0.34        27\n",
      "\n",
      "    accuracy                           0.95       685\n",
      "   macro avg       0.66      0.65      0.66       685\n",
      "weighted avg       0.95      0.95      0.95       685\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ROC Curve plot for ADASYN saved as roc_curve_ADASYN_HDBSCAN.png\n",
      "Using BorderlineSMOTE...\n",
      "Fitting 2 folds for each of 64 candidates, totalling 128 fits\n",
      "[LightGBM] [Info] Number of positive: 1578, number of negative: 2630\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000312 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1263\n",
      "[LightGBM] [Info] Number of data points in the train set: 4208, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'ensemble__lightgbm__learning_rate': 0.01, 'ensemble__lightgbm__max_depth': 10, 'ensemble__lightgbm__n_estimators': 200, 'ensemble__lightgbm__num_leaves': 31, 'ensemble__random_forest__max_depth': 20, 'ensemble__random_forest__n_estimators': 250}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.98       658\n",
      "         1.0       0.73      0.30      0.42        27\n",
      "\n",
      "    accuracy                           0.97       685\n",
      "   macro avg       0.85      0.65      0.70       685\n",
      "weighted avg       0.96      0.97      0.96       685\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ROC Curve plot for BorderlineSMOTE saved as roc_curve_BorderlineSMOTE_HDBSCAN.png\n",
      "Using SVMSMOTE...\n",
      "Fitting 2 folds for each of 64 candidates, totalling 128 fits\n",
      "[LightGBM] [Info] Number of positive: 1578, number of negative: 2630\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1238\n",
      "[LightGBM] [Info] Number of data points in the train set: 4208, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'ensemble__lightgbm__learning_rate': 0.01, 'ensemble__lightgbm__max_depth': 10, 'ensemble__lightgbm__n_estimators': 200, 'ensemble__lightgbm__num_leaves': 50, 'ensemble__random_forest__max_depth': 10, 'ensemble__random_forest__n_estimators': 100}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98       658\n",
      "         1.0       0.67      0.30      0.41        27\n",
      "\n",
      "    accuracy                           0.97       685\n",
      "   macro avg       0.82      0.65      0.70       685\n",
      "weighted avg       0.96      0.97      0.96       685\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ROC Curve plot for SVMSMOTE saved as roc_curve_SVMSMOTE_HDBSCAN.png\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, SMOTENC, SMOTEN, ADASYN, BorderlineSMOTE, SVMSMOTE\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Select only the top features\n",
    "selected_features = ['SBP3', 'DBP4', 'MAT_WEIGHT', 'DBP2', 'DBP3']\n",
    "X_train_selected = X_train_Clustering_scaled_df[selected_features]\n",
    "X_test_selected = X_test_Clustering_scaled_df[selected_features]\n",
    "\n",
    "# Define oversampling methods to try\n",
    "over_sampling_methods = {\n",
    "    'RandomOverSampler': RandomOverSampler(random_state=42),\n",
    "    'SMOTE': SMOTE(random_state=42, sampling_strategy=0.6),\n",
    "    'SMOTENC': SMOTENC(categorical_features=[4], random_state=42, sampling_strategy=0.6),\n",
    "    'SMOTEN': SMOTEN(random_state=42, sampling_strategy=0.6),\n",
    "    'ADASYN': ADASYN(random_state=42, sampling_strategy=0.6),\n",
    "    'BorderlineSMOTE': BorderlineSMOTE(random_state=42, sampling_strategy=0.6),\n",
    "    'SVMSMOTE': SVMSMOTE(random_state=42, sampling_strategy=0.6)\n",
    "}\n",
    "\n",
    "# Loop over each oversampling method\n",
    "for name, sampler in over_sampling_methods.items():\n",
    "    print(f\"Using {name}...\")\n",
    "\n",
    "    # Define the base models for the ensemble\n",
    "    model1 = LGBMClassifier(random_state=42, class_weight='balanced')\n",
    "    model2 = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "    # Create an ensemble model using VotingClassifier (soft voting)\n",
    "    ensemble_model = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lightgbm', model1),\n",
    "            ('random_forest', model2)\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "\n",
    "    # Define the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('sampler', sampler),\n",
    "        ('ensemble', ensemble_model)\n",
    "    ])\n",
    "\n",
    "    # Define hyperparameter grid for tuning\n",
    "    param_grid = {\n",
    "        'ensemble__lightgbm__num_leaves': [31, 50],\n",
    "        'ensemble__lightgbm__max_depth': [-1, 10],\n",
    "        'ensemble__lightgbm__learning_rate': [0.01, 0.1],\n",
    "        'ensemble__lightgbm__n_estimators': [100, 200],\n",
    "        'ensemble__random_forest__n_estimators': [100, 250],\n",
    "        'ensemble__random_forest__max_depth': [10, 20]\n",
    "    }\n",
    "\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring='recall',\n",
    "        cv=2,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Train the model with hyperparameter tuning\n",
    "    grid_search.fit(X_train_selected, y_train_Clustering)\n",
    "\n",
    "    # Get the best model and parameters\n",
    "    print(\"\\nBest Parameters from GridSearchCV:\")\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # Use the best model to make predictions\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test_selected)\n",
    "    y_prob = grid_search.best_estimator_.predict_proba(X_test_selected)[:, 1]  # Get probability for positive class\n",
    "\n",
    "    # Metrics\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_Clustering, y_pred))\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_Clustering, y_prob)\n",
    "    auc_score = roc_auc_score(y_test_Clustering, y_prob)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='b', label=f'ROC Curve (AUC = {auc_score:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='r', linestyle='--')  # Diagonal line\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve for {name}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(f'roc_curve_{name}_HDBSCAN.png')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"ROC Curve plot for {name} saved as roc_curve_{name}_HDBSCAN.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfffd0d1-fe05-4f09-8612-dcd554866375",
   "metadata": {},
   "source": [
    "### IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4525a258-e962-4dbf-8b61-00e5e64564cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to remove outliers per class using IQR\n",
    "def remove_outliers_per_class_iqr(df, class_col):\n",
    "    cleaned_dfs = []\n",
    "    classes = df[class_col].unique()  # Get unique classes in the target column\n",
    "    \n",
    "    for cls in classes:\n",
    "        # Filter data for the current class\n",
    "        class_data = df[df[class_col] == cls]\n",
    "        features = class_data.drop(columns=[class_col])  # Exclude the class column\n",
    "        \n",
    "        # Calculate the IQR for each feature\n",
    "        Q1 = features.quantile(0.25)\n",
    "        Q3 = features.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Define the bounds for outliers\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Identify outliers and keep inliers (values within bounds)\n",
    "        outliers = (features < lower_bound) | (features > upper_bound)\n",
    "        \n",
    "        # Keep only inliers (rows without outliers)\n",
    "        class_clean = class_data[~outliers.any(axis=1)]  # Drop rows with any outliers\n",
    "        cleaned_dfs.append(class_clean)\n",
    "    \n",
    "    # Combine cleaned data from all classes\n",
    "    return pd.concat(cleaned_dfs)\n",
    "\n",
    "# Apply the function to your DataFrame (replace 'APH' with your actual class column name)\n",
    "df_clean = remove_outliers_per_class_iqr(df, class_col='APH')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d5c35d0-fc21-4303-844b-63ceea1d14ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "APH\n",
       "0.0    1823\n",
       "1.0      47\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean['APH'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db315e-f789-41d3-9659-706ddb42bcbd",
   "metadata": {},
   "source": [
    "### IQR -- Test / Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1dea194-a07a-4aaf-a91f-3b91c7d84162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1496, 24)\n",
      "X_test shape: (374, 24)\n",
      "y_train shape: (1496,)\n",
      "y_test shape: (374,)\n",
      "\n",
      "Class distribution in y_train:\n",
      "APH\n",
      "0.0    0.974599\n",
      "1.0    0.025401\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution in y_test:\n",
      "APH\n",
      "0.0    0.975936\n",
      "1.0    0.024064\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the target variable and features\n",
    "target = 'APH'  # Replace 'APH' with the actual target column name if different\n",
    "features = [col for col in df_clean.columns if col != target]  # All columns except the target\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = df_clean[features]\n",
    "y = df_clean[target]\n",
    "\n",
    "# Perform the stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Check class distribution in the train and test splits\n",
    "print(\"\\nClass distribution in y_train:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nClass distribution in y_test:\")\n",
    "print(y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9246636c-78a9-450d-a589-0056b9f65cfb",
   "metadata": {},
   "source": [
    "### Selecting Best Features --- IQR Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "35813151-2178-4c90-9498-ac2e15b6fc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information Scores:\n",
      "          Feature  Mutual Information\n",
      "16           SBP3            0.053132\n",
      "20           DBP4            0.038337\n",
      "14           DBP2            0.016465\n",
      "17           DBP3            0.010171\n",
      "11           DBP1            0.008805\n",
      "9          PARITY            0.008375\n",
      "13           SBP2            0.008193\n",
      "7     SINGLE_TWIN            0.007666\n",
      "8       GRAVIDITY            0.006115\n",
      "19           SBP4            0.005609\n",
      "3        PREV_MIS            0.004824\n",
      "22     MAT_WEIGHT            0.004798\n",
      "5     PREV_MULTIP            0.003724\n",
      "4        PREV_PTB            0.000893\n",
      "10           SBP1            0.000399\n",
      "21     UDIP_PROT4            0.000000\n",
      "18     UDIP_PROT3            0.000000\n",
      "0          PW_AGE            0.000000\n",
      "12     UDIP_PROT1            0.000000\n",
      "15     UDIP_PROT2            0.000000\n",
      "1    PW_EDUCATION            0.000000\n",
      "6         PREV_CS            0.000000\n",
      "2         PREV_SB            0.000000\n",
      "23  LABOUR_24_1.0            0.000000\n",
      "\n",
      "Top Features based on Mutual Information:\n",
      "['SBP3', 'DBP4', 'DBP2', 'DBP3', 'DBP1', 'PARITY']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming `X_train` is your feature matrix and `y_train` is your target variable\n",
    "# Example (replace this with your actual data):\n",
    "import numpy as np\n",
    "\n",
    "# Compute Mutual Information\n",
    "mi_scores = mutual_info_classif(X_train, y_train, random_state=42)\n",
    "\n",
    "# Create a DataFrame for easier manipulation and sorting\n",
    "mi_df = pd.DataFrame({'Feature': X_train.columns, 'Mutual Information': mi_scores})\n",
    "mi_df = mi_df.sort_values(by='Mutual Information', ascending=False)\n",
    "\n",
    "# Display top features based on Mutual Information\n",
    "print(\"Mutual Information Scores:\")\n",
    "print(mi_df)\n",
    "\n",
    "# Select top N features (e.g., top 5)\n",
    "top_features = mi_df.head(6)['Feature'].tolist()\n",
    "print(\"\\nTop Features based on Mutual Information:\")\n",
    "print(top_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f8ebc7-c506-44ab-bd4a-3ed1062550a4",
   "metadata": {},
   "source": [
    "### Random Forest with different Oversampling technique IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2fb89aea-bc93-4d04-813f-07555275d617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RandomOverSampler...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'rf__max_depth': 100, 'rf__n_estimators': 100}\n",
      "\n",
      "Confusion Matrix:\n",
      "[[365   0]\n",
      " [  3   6]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00       365\n",
      "         1.0       1.00      0.67      0.80         9\n",
      "\n",
      "    accuracy                           0.99       374\n",
      "   macro avg       1.00      0.83      0.90       374\n",
      "weighted avg       0.99      0.99      0.99       374\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Using SMOTE...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'rf__max_depth': 100, 'rf__n_estimators': 250}\n",
      "\n",
      "Confusion Matrix:\n",
      "[[363   2]\n",
      " [  3   6]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       365\n",
      "         1.0       0.75      0.67      0.71         9\n",
      "\n",
      "    accuracy                           0.99       374\n",
      "   macro avg       0.87      0.83      0.85       374\n",
      "weighted avg       0.99      0.99      0.99       374\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Using SMOTENC...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'rf__max_depth': 100, 'rf__n_estimators': 250}\n",
      "\n",
      "Confusion Matrix:\n",
      "[[360   5]\n",
      " [  3   6]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       365\n",
      "         1.0       0.55      0.67      0.60         9\n",
      "\n",
      "    accuracy                           0.98       374\n",
      "   macro avg       0.77      0.83      0.79       374\n",
      "weighted avg       0.98      0.98      0.98       374\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Using SMOTEN...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'rf__max_depth': 100, 'rf__n_estimators': 100}\n",
      "\n",
      "Confusion Matrix:\n",
      "[[365   0]\n",
      " [  4   5]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99       365\n",
      "         1.0       1.00      0.56      0.71         9\n",
      "\n",
      "    accuracy                           0.99       374\n",
      "   macro avg       0.99      0.78      0.85       374\n",
      "weighted avg       0.99      0.99      0.99       374\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Using ADASYN...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'rf__max_depth': 100, 'rf__n_estimators': 100}\n",
      "\n",
      "Confusion Matrix:\n",
      "[[363   2]\n",
      " [  3   6]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       365\n",
      "         1.0       0.75      0.67      0.71         9\n",
      "\n",
      "    accuracy                           0.99       374\n",
      "   macro avg       0.87      0.83      0.85       374\n",
      "weighted avg       0.99      0.99      0.99       374\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Using BorderlineSMOTE...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'rf__max_depth': 100, 'rf__n_estimators': 100}\n",
      "\n",
      "Confusion Matrix:\n",
      "[[365   0]\n",
      " [  4   5]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99       365\n",
      "         1.0       1.00      0.56      0.71         9\n",
      "\n",
      "    accuracy                           0.99       374\n",
      "   macro avg       0.99      0.78      0.85       374\n",
      "weighted avg       0.99      0.99      0.99       374\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Using SVMSMOTE...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'rf__max_depth': 100, 'rf__n_estimators': 100}\n",
      "\n",
      "Confusion Matrix:\n",
      "[[365   0]\n",
      " [  4   5]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99       365\n",
      "         1.0       1.00      0.56      0.71         9\n",
      "\n",
      "    accuracy                           0.99       374\n",
      "   macro avg       0.99      0.78      0.85       374\n",
      "weighted avg       0.99      0.99      0.99       374\n",
      "\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, SMOTENC, SMOTEN, ADASYN, BorderlineSMOTE, KMeansSMOTE, SVMSMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming you already have X_train, X_test, y_train, y_test\n",
    "\n",
    "# Select only the top features\n",
    "selected_features = ['SBP3', 'DBP4', 'DBP2', 'DBP3', 'DBP1', 'PARITY', 'SBP2']\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Define oversampling methods to try\n",
    "oversampling_methods = {\n",
    "    'RandomOverSampler': RandomOverSampler(random_state=42),\n",
    "    'SMOTE': SMOTE(random_state=42, sampling_strategy=0.8),\n",
    "    'SMOTENC': SMOTENC(categorical_features=[4], random_state=42, sampling_strategy=0.8),  # Assume 'PARITY' is categorical\n",
    "    'SMOTEN': SMOTEN(random_state=42, sampling_strategy=0.8),\n",
    "    'ADASYN': ADASYN(random_state=42, sampling_strategy=0.8),\n",
    "    'BorderlineSMOTE': BorderlineSMOTE(random_state=42, sampling_strategy=0.8),\n",
    "    'SVMSMOTE': SVMSMOTE(random_state=42, sampling_strategy=0.8)\n",
    "}\n",
    "\n",
    "# Loop over each oversampling method\n",
    "for name, sampler in oversampling_methods.items():\n",
    "    print(f\"Using {name}...\")\n",
    "\n",
    "    # Define the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Scaling step\n",
    "        ('smote', sampler),            # Different oversampling methods\n",
    "        ('rf', RandomForestClassifier(random_state=42, class_weight='balanced'))  # Random Forest with custom class weights\n",
    "    ])\n",
    "\n",
    "    # Define hyperparameter grid for tuning 'n_estimators'\n",
    "    param_grid = {\n",
    "        'rf__n_estimators': [100, 250, 500],\n",
    "        'rf__max_depth': [100]  # Trying different values for max_depth\n",
    "    }\n",
    "\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring='recall',        # Use recall to evaluate the performance\n",
    "        cv=3,                    # Cross-validation with 3 folds\n",
    "        n_jobs=-1,               # Use all CPU cores for computation\n",
    "        verbose=2                # Show detailed progress\n",
    "    )\n",
    "\n",
    "    # Train the model with hyperparameter tuning\n",
    "    grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Get the best model and parameters\n",
    "    print(\"\\nBest Parameters from GridSearchCV:\")\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # Use the best model to make predictions\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test_selected)\n",
    "\n",
    "    # Metrics\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\" + \"=\"*80)  # Print a separator between different methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee86def-6eed-4d3e-a769-2513b2dcf0b9",
   "metadata": {},
   "source": [
    "### Ensembled model of the EasyEnsembleClassifier,RUSBoostClassifier,BalancedBaggingClassifier,BalancedRandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ea86d8f-15dd-49bc-8041-7aedf8ab5b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble Classification Report (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.77      0.87       365\n",
      "         1.0       0.08      0.78      0.14         9\n",
      "\n",
      "    accuracy                           0.77       374\n",
      "   macro avg       0.53      0.77      0.50       374\n",
      "weighted avg       0.97      0.77      0.85       374\n",
      "\n",
      "\n",
      "Ensemble Confusion Matrix (Test Data):\n",
      "[[281  84]\n",
      " [  2   7]]\n",
      "ROC AUC Score: 0.8983257229832572\n",
      "\n",
      "Confusion Matrix plot saved at: ensemble_confusion_matrix.png\n",
      "ROC Curve plot saved at: ensemble_roc_curve.png\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from imblearn.ensemble import EasyEnsembleClassifier, RUSBoostClassifier, BalancedBaggingClassifier, BalancedRandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "# Select the top features (assuming they are in the dataset)\n",
    "selected_features = ['SBP3', 'DBP4', 'DBP2', 'DBP3', 'DBP1', 'PARITY', 'SBP2']\n",
    "\n",
    "# Select only the relevant features from X_train and X_test\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# Define individual models\n",
    "models = {\n",
    "    'EasyEnsembleClassifier': EasyEnsembleClassifier(random_state=42),\n",
    "    'RUSBoostClassifier': RUSBoostClassifier(random_state=42),\n",
    "    'BalancedBaggingClassifier': BalancedBaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=10), \n",
    "        n_estimators=100, \n",
    "        random_state=42\n",
    "    ),\n",
    "    'BalancedRandomForestClassifier': BalancedRandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Create an ensemble model using Voting Classifier (Majority Voting)\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('easy_ensemble', models['EasyEnsembleClassifier']),\n",
    "    ('rusboost', models['RUSBoostClassifier']),\n",
    "    ('balanced_bagging', models['BalancedBaggingClassifier']),\n",
    "    ('balanced_rf', models['BalancedRandomForestClassifier'])\n",
    "], voting='soft')  # Use 'soft' voting for probability averaging\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions using the ensemble model\n",
    "y_pred_train_ensemble = ensemble_model.predict(X_train_scaled)\n",
    "y_pred_test_ensemble = ensemble_model.predict(X_test_scaled)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nEnsemble Classification Report (Test Data):\")\n",
    "print(classification_report(y_test, y_pred_test_ensemble))\n",
    "\n",
    "# Confusion Matrix for Ensemble Test Data\n",
    "cm = confusion_matrix(y_test, y_pred_test_ensemble)\n",
    "print(\"\\nEnsemble Confusion Matrix (Test Data):\")\n",
    "print(cm)\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, ensemble_model.predict_proba(X_test_scaled)[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(f\"ROC AUC Score: {roc_auc}\")\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.title(\"Ensemble Confusion Matrix\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save Confusion Matrix plot\n",
    "confusion_matrix_file = 'ensemble_confusion_matrix.png'\n",
    "plt.savefig(confusion_matrix_file)\n",
    "plt.close()\n",
    "\n",
    "# Plotting ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line (random classifier)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save ROC Curve plot\n",
    "roc_curve_file = 'ensemble_roc_curve.png'\n",
    "plt.savefig(roc_curve_file)\n",
    "plt.close()\n",
    "\n",
    "# Print file paths for downloading\n",
    "print(f\"\\nConfusion Matrix plot saved at: {confusion_matrix_file}\")\n",
    "print(f\"ROC Curve plot saved at: {roc_curve_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c0244e-ab8d-4bea-afab-6fa6e372a8f8",
   "metadata": {},
   "source": [
    "## Best Model All the sampling techniques with the lightbgm and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5ca135d-21d5-4146-aa4c-df9dbe89aeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RandomOverSampler...\n",
      "Fitting 2 folds for each of 64 candidates, totalling 128 fits\n",
      "[LightGBM] [Info] Number of positive: 1458, number of negative: 1458\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 249\n",
      "[LightGBM] [Info] Number of data points in the train set: 2916, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'ensemble__lightgbm__learning_rate': 0.1, 'ensemble__lightgbm__max_depth': -1, 'ensemble__lightgbm__n_estimators': 100, 'ensemble__lightgbm__num_leaves': 31, 'ensemble__random_forest__max_depth': 10, 'ensemble__random_forest__n_estimators': 100}\n",
      "\n",
      "Confusion Matrix:\n",
      "[[363   2]\n",
      " [  3   6]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       365\n",
      "         1.0       0.75      0.67      0.71         9\n",
      "\n",
      "    accuracy                           0.99       374\n",
      "   macro avg       0.87      0.83      0.85       374\n",
      "weighted avg       0.99      0.99      0.99       374\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Using SMOTE...\n",
      "Fitting 2 folds for each of 64 candidates, totalling 128 fits\n",
      "[LightGBM] [Info] Number of positive: 874, number of negative: 1458\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000361 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1660\n",
      "[LightGBM] [Info] Number of data points in the train set: 2332, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'ensemble__lightgbm__learning_rate': 0.01, 'ensemble__lightgbm__max_depth': -1, 'ensemble__lightgbm__n_estimators': 100, 'ensemble__lightgbm__num_leaves': 31, 'ensemble__random_forest__max_depth': 10, 'ensemble__random_forest__n_estimators': 100}\n",
      "\n",
      "Confusion Matrix:\n",
      "[[358   7]\n",
      " [  3   6]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.99       365\n",
      "         1.0       0.46      0.67      0.55         9\n",
      "\n",
      "    accuracy                           0.97       374\n",
      "   macro avg       0.73      0.82      0.77       374\n",
      "weighted avg       0.98      0.97      0.98       374\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Using SMOTENC...\n",
      "Fitting 2 folds for each of 64 candidates, totalling 128 fits\n",
      "[LightGBM] [Info] Number of positive: 874, number of negative: 1458\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1426\n",
      "[LightGBM] [Info] Number of data points in the train set: 2332, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'ensemble__lightgbm__learning_rate': 0.01, 'ensemble__lightgbm__max_depth': -1, 'ensemble__lightgbm__n_estimators': 200, 'ensemble__lightgbm__num_leaves': 31, 'ensemble__random_forest__max_depth': 10, 'ensemble__random_forest__n_estimators': 100}\n",
      "\n",
      "Confusion Matrix:\n",
      "[[357   8]\n",
      " [  3   6]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.98       365\n",
      "         1.0       0.43      0.67      0.52         9\n",
      "\n",
      "    accuracy                           0.97       374\n",
      "   macro avg       0.71      0.82      0.75       374\n",
      "weighted avg       0.98      0.97      0.97       374\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Using SMOTEN...\n",
      "Fitting 2 folds for each of 64 candidates, totalling 128 fits\n",
      "[LightGBM] [Info] Number of positive: 874, number of negative: 1458\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000312 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 249\n",
      "[LightGBM] [Info] Number of data points in the train set: 2332, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'ensemble__lightgbm__learning_rate': 0.01, 'ensemble__lightgbm__max_depth': -1, 'ensemble__lightgbm__n_estimators': 100, 'ensemble__lightgbm__num_leaves': 31, 'ensemble__random_forest__max_depth': 10, 'ensemble__random_forest__n_estimators': 250}\n",
      "\n",
      "Confusion Matrix:\n",
      "[[362   3]\n",
      " [  4   5]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       365\n",
      "         1.0       0.62      0.56      0.59         9\n",
      "\n",
      "    accuracy                           0.98       374\n",
      "   macro avg       0.81      0.77      0.79       374\n",
      "weighted avg       0.98      0.98      0.98       374\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Using ADASYN...\n",
      "Fitting 2 folds for each of 64 candidates, totalling 128 fits\n",
      "[LightGBM] [Info] Number of positive: 883, number of negative: 1458\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000375 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1668\n",
      "[LightGBM] [Info] Number of data points in the train set: 2341, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'ensemble__lightgbm__learning_rate': 0.01, 'ensemble__lightgbm__max_depth': -1, 'ensemble__lightgbm__n_estimators': 100, 'ensemble__lightgbm__num_leaves': 31, 'ensemble__random_forest__max_depth': 10, 'ensemble__random_forest__n_estimators': 100}\n",
      "\n",
      "Confusion Matrix:\n",
      "[[357   8]\n",
      " [  3   6]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.98       365\n",
      "         1.0       0.43      0.67      0.52         9\n",
      "\n",
      "    accuracy                           0.97       374\n",
      "   macro avg       0.71      0.82      0.75       374\n",
      "weighted avg       0.98      0.97      0.97       374\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Using BorderlineSMOTE...\n",
      "Fitting 2 folds for each of 64 candidates, totalling 128 fits\n",
      "[LightGBM] [Info] Number of positive: 874, number of negative: 1458\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1606\n",
      "[LightGBM] [Info] Number of data points in the train set: 2332, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'ensemble__lightgbm__learning_rate': 0.01, 'ensemble__lightgbm__max_depth': -1, 'ensemble__lightgbm__n_estimators': 100, 'ensemble__lightgbm__num_leaves': 31, 'ensemble__random_forest__max_depth': 10, 'ensemble__random_forest__n_estimators': 100}\n",
      "\n",
      "Confusion Matrix:\n",
      "[[360   5]\n",
      " [  4   5]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       365\n",
      "         1.0       0.50      0.56      0.53         9\n",
      "\n",
      "    accuracy                           0.98       374\n",
      "   macro avg       0.74      0.77      0.76       374\n",
      "weighted avg       0.98      0.98      0.98       374\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Using SVMSMOTE...\n",
      "Fitting 2 folds for each of 64 candidates, totalling 128 fits\n",
      "[LightGBM] [Info] Number of positive: 498, number of negative: 1458\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000394 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1487\n",
      "[LightGBM] [Info] Number of data points in the train set: 1956, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'ensemble__lightgbm__learning_rate': 0.01, 'ensemble__lightgbm__max_depth': -1, 'ensemble__lightgbm__n_estimators': 100, 'ensemble__lightgbm__num_leaves': 31, 'ensemble__random_forest__max_depth': 10, 'ensemble__random_forest__n_estimators': 100}\n",
      "\n",
      "Confusion Matrix:\n",
      "[[359   6]\n",
      " [  4   5]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.99       365\n",
      "         1.0       0.45      0.56      0.50         9\n",
      "\n",
      "    accuracy                           0.97       374\n",
      "   macro avg       0.72      0.77      0.74       374\n",
      "weighted avg       0.98      0.97      0.97       374\n",
      "\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, SMOTENC, SMOTEN, ADASYN, BorderlineSMOTE, SVMSMOTE\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming you already have X_train, X_test, y_train, y_test\n",
    "\n",
    "# Select only the top features\n",
    "selected_features = ['SBP3', 'DBP4', 'DBP2', 'DBP3', 'DBP1', 'PARITY', 'SBP2']\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Define oversampling methods to try\n",
    "oversampling_methods = {\n",
    "    'RandomOverSampler': RandomOverSampler(random_state=42),\n",
    "    'SMOTE': SMOTE(random_state=42, sampling_strategy=0.6),  # Reduced sampling strategy for faster processing\n",
    "    'SMOTENC': SMOTENC(categorical_features=[4], random_state=42, sampling_strategy=0.6),\n",
    "    'SMOTEN': SMOTEN(random_state=42, sampling_strategy=0.6),\n",
    "    'ADASYN': ADASYN(random_state=42, sampling_strategy=0.6),\n",
    "    'BorderlineSMOTE': BorderlineSMOTE(random_state=42, sampling_strategy=0.6),\n",
    "    'SVMSMOTE': SVMSMOTE(random_state=42, sampling_strategy=0.6)\n",
    "}\n",
    "\n",
    "# Loop over each oversampling method\n",
    "for name, sampler in oversampling_methods.items():\n",
    "    print(f\"Using {name}...\")\n",
    "\n",
    "    # Define the base models for the ensemble\n",
    "    model1 = LGBMClassifier(random_state=42, class_weight='balanced')\n",
    "    model2 = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "    # Create an ensemble model using VotingClassifier (soft voting)\n",
    "    ensemble_model = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lightgbm', model1),\n",
    "            ('random_forest', model2)\n",
    "        ],\n",
    "        voting='soft'  # Soft voting: average predicted probabilities\n",
    "    )\n",
    "\n",
    "    # Define the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Scaling step\n",
    "        ('smote', sampler),            # Different oversampling methods\n",
    "        ('ensemble', ensemble_model)   # The ensemble model\n",
    "    ])\n",
    "\n",
    "    # Define hyperparameter grid for tuning ensemble model's parameters\n",
    "    param_grid = {\n",
    "        'ensemble__lightgbm__num_leaves': [31, 50],  # Reduced grid\n",
    "        'ensemble__lightgbm__max_depth': [-1, 10],\n",
    "        'ensemble__lightgbm__learning_rate': [0.01, 0.1],\n",
    "        'ensemble__lightgbm__n_estimators': [100, 200],\n",
    "        \n",
    "        'ensemble__random_forest__n_estimators': [100, 250],  # Reduced grid\n",
    "        'ensemble__random_forest__max_depth': [10, 20]\n",
    "    }\n",
    "\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring='recall',        # Use recall to evaluate the performance\n",
    "        cv=2,                    # Reduced cross-validation folds\n",
    "        n_jobs=-1,               # Use all CPU cores for computation\n",
    "        verbose=1                # Show detailed progress\n",
    "    )\n",
    "\n",
    "    # Train the model with hyperparameter tuning\n",
    "    grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Get the best model and parameters\n",
    "    print(\"\\nBest Parameters from GridSearchCV:\")\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # Use the best model to make predictions\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test_selected)\n",
    "\n",
    "    # Metrics\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\" + \"=\"*80)  # Print a separator between different methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4af3d0-48bd-483c-bf8b-1f55f9bc1179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
